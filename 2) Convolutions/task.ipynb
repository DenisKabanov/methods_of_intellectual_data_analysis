{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Реализовать стандартную функцию свертки​.\n",
    "2) Реализовать функцию свертки через im2col.\n",
    "3) Реализовать Depthwise-separable свертку.\n",
    "\n",
    "* Специфицировать размер, количество фильтров, входной tensor, stride, padding...\n",
    "* Придумать тесты для проверки работоспособности всех вариантов свёртки.​"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Настройки/Импорты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # для работы с массивами\n",
    "import torch # для работы с тензорами\n",
    "import torch.nn.functional as F # для функций с тензорами\n",
    "import torchvision.io as tv # для работы с изображениями\n",
    "from tqdm import tqdm # для отслеживания прогресса\n",
    "from matplotlib import pyplot as plt # для построения графиков/вывода изображений"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Вспомогательные функции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image(image, figsize: tuple, gray: bool=False) -> None:\n",
    "    \"\"\"\n",
    "    Функция для вывода изображений.\\n\n",
    "    Parameters:\n",
    "        * image: изображение, что нужно вывести\n",
    "        * figsize: размер выводимой картинки\n",
    "        * gray: флаг, является ли изображение одноканальным (чёрно-белым)\\n\n",
    "    Returns:\n",
    "        * None\n",
    "    \"\"\"\n",
    "    cmap = \"gray\" if gray else \"viridis\" # выбор цветовой схемы, (стандартная — \"viridis\", но если передан ключ gray — \"gray\")\n",
    "\n",
    "    plt.figure(figsize=figsize) # размер фигуры\n",
    "    plt.imshow(image, cmap) # вывод изображения\n",
    "    plt.show() # показ фигуры"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_conv(func, input) -> None:\n",
    "    \"\"\"\n",
    "    Функция для тестирования работы свёрток. Сравнивает с реализацией свёрточного слоя на PyTorch при различных параметрах.\\n\n",
    "    Parameters:\n",
    "        * func: функция свёртки, что нужно протестировать\n",
    "        * input: входной массив, на котором будет проводиться тестирование (в формате float32) размерности (Channels, Height, Width) ~ (Channels, H_in, W_in)\\n \n",
    "    Returns:\n",
    "        * None: выдаст Exception в случае, если есть несовпадение ответов у проверяемой и контрольной функции свёртки\n",
    "    \"\"\"\n",
    "    Channels, H_in, W_in = input.shape # берём значения Channels, Height, Width из входной матрицы\n",
    "\n",
    "    for Num_filters in tqdm([1, 2]): # рассматриваем возможное число фильтров\n",
    "        for kernel_size in [1, 2, 3, 5, 7]: # рассматриваем возможные размеры ядер\n",
    "\n",
    "            W = torch.randint(low=0, high=10, size=(Num_filters, Channels, kernel_size, kernel_size), dtype=torch.float32) # случайно генерируем веса для ядер свёртки (int от 0 до 10 — чтобы решение всегда было точным с последующей конвертацией в float32)\n",
    "            # размерность W: (число ядер(фильтров) на слое свёртки, Channels — число входных каналов, высота ядра свёртки H_conv, ширина ядра свёртки W_conv)\n",
    "            b = torch.randint(low=0, high=10, size=(Num_filters,), dtype=torch.float32) # bias по аналогии\n",
    "            # размерность b: (число ядер(фильтров) на слое свёртки)\n",
    "\n",
    "            for stride in [1, 2, 5]: # рассматриваем варианты параметра stride\n",
    "                for padding in [0, 1, 2]: # рассматриваем варианты параметра padding\n",
    "                    real_conv = torch.nn.Conv2d(in_channels=Channels, out_channels=Num_filters, kernel_size=kernel_size, stride=stride, padding=padding) # слой свёртки из PyTorch\n",
    "                    real_conv.weight.data = W # меняем веса ядер свёртки на тестируемые (со случайных, заданных при создании Conv2d)\n",
    "                    real_conv.bias.data = b # меняем bias ядер свёртки на тестируемые (со случайных, заданных при создании Conv2d)\n",
    "                    \n",
    "                    res_real = real_conv(input) # считаем результат контрольной свёртки\n",
    "                    \n",
    "                    res_func = func(input, W, b, stride, padding) # считаем результат проверяемой свёртки\n",
    "\n",
    "                    if not torch.equal(res_func, res_real): # сравниваем, что все значения в тензорах одинаковые\n",
    "                        raise Exception(f\"Результат свёрток не совпал при параметрах: Num_filters={Num_filters}, kernel_size={kernel_size}, stride={stride}, padding={padding}\") # если есть несовпадения — выкидываем исключение\n",
    "    print(\"Все тесты прошли успешно!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Реализация стандартной свёртки."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/conv_simple.png\" alt=\"Simple convolution\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция свёртки:\n",
    "$$\n",
    "\\mathbf{Out}[id_{conv}][x][y] = \\sum_{i=0}^{H_{conv}-1}\\sum_{j=0}^{W_{conv}-1}\\sum_{c=0}^{Channels-1}\\mathbf{Input}[c][Sx+i][Sy+j] * \\mathbf{W}[id_{conv}][c][i][j] + \\mathbf{b}[id_{conv}]\n",
    "$$\n",
    "Где:\n",
    "* Input — входная матрица размерности (Channels, Height, Width) ~ (Channels, $H_{in}$, $W_{in}$)\n",
    "* W — матрица весов свёрточного слоя размерности (Num_filters, Channels, $H_{conv}$, $W_{conv}$)\n",
    "* b — смещения (bias) для вильтров свёртки (одно число на фильтр) размерности (Num_filters)\n",
    "* Out — выходная матрица (Num_filters, $H_{out}$, $W_{out}$), где $H_{out}$, $W_{out}$ зависят от параметров свёртки\n",
    "* $id_{conv}$  — номер рассматриваемой свёртки (фильтра свёртки), изменяется от 0 до Num_filters-1\n",
    "* x — координата **нового** пикселя по вертикали ~ номер строки (в изначальном изображении — Height ~ $H_{in}$)\n",
    "* y — координата **нового** пикселя по горизонтали ~ номер столбца (в изначальном изображении — Width ~ $W_{in}$)\n",
    "* $H_{conv}$ — размер ядра свёртки по вертикали (итерируемся с помощью *i*)\n",
    "* $W_{conv}$ — размер ядра свёртки по горизонтали (итерируемся с помощью *j*)\n",
    "* Channels — число каналов (фильтров) у входного тензора (итерируемся с помощью *c*)\n",
    "* S — значение параметра \"Stride\", отвечающее за размер шага по изображению (смещение по изображению)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_simple(Input, W, b, stride: int=1, padding: int=0) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Функция простой свёртки.\\n\n",
    "    Parameters:\n",
    "        * Input: входной массив размерности (Channels, Height, Width) ~ (Channels, H_in, W_in)\n",
    "        * W: веса для ядер свёртки\n",
    "        * b: bias, смещение для фильтра свёртки\n",
    "        * stride: размер шага по осям (одинаков как для оси OX, так и для OY)\n",
    "        * padding: значение количества добавляемх \"размерностей\" по краям входного массива, не включая размерность, отвечающую за каналы (одинаков как для оси OX, так и для OY)\\n\n",
    "    Returns:\n",
    "        * torch.Tensor: результат применения свёртки\n",
    "    \"\"\"\n",
    "    Channels, H_in, W_in = Input.shape # берём значения Channels, Height, Width из входной матрицы\n",
    "    Num_filters, Channels, H_conv, W_conv = W.shape # берём значения Num_filters, Channels, H_conv, W_conv из матрицы весов свёртки (Channels должен совпадать с аналогичной размерность из Input)\n",
    "\n",
    "    Input_padded = F.pad(input=Input, pad=(padding, padding, padding, padding, 0, 0), mode='constant', value=0) # добавляем padding из value на входной массив (0,0 — отвечают за добавляемую размерность в Channels (до и после исходных данных), остальные по аналогии за Width и Height)\n",
    "\n",
    "    H_out = int((H_in + 2*padding - (H_conv-1) - 1) / stride + 1) # Height выходного тензора\n",
    "    W_out = int((W_in + 2*padding - (W_conv-1) - 1) / stride + 1) # Width выходного тензора\n",
    "    Out = torch.zeros(size=(Num_filters, H_out, W_out), dtype=torch.float32) # заготовка под выходной тензор, заполненная нулями\n",
    "    \n",
    "    # print(f\"Input_padded shape: {Input_padded.shape}\")\n",
    "    # print(f\"Out shape: {Out.shape}\")\n",
    "\n",
    "    for id_filter in range(Num_filters): # идём по числу фильтров в одной свёртке (range вернёт список чисел от 0 до Num_filters-1 включительно)\n",
    "        for x in range(H_out): # идём по пикселям высоты выходного тензора\n",
    "            for y in range(W_out): # идём по пикселям ширины выходного тензора\n",
    "#=========================== вариант через медленное сложение всех элементов ========================\n",
    "                # for i in range(H_conv): # идём по высоте фильтра (ядра) свёртки\n",
    "                #     for j in range(W_conv): # идём по ширине фильтра (ядра) свёртки\n",
    "                #         for c in range(Channels): # идём по каналам входного изображения\n",
    "                #             Out[id_filter][x][y] += Input_padded[c][stride*x + i][stride*y + j] * W [id_filter][c][i][j]\n",
    "#--------------------------- вариант через быстрое перемножение тензоров ----------------------------\n",
    "                H_from, H_to = stride*x, stride*x+H_conv # с какого пикселя по какой смотреть на входном изображении по вертикали\n",
    "                W_from, W_to = stride*y, stride*y+W_conv # с какого пикселя по какой смотреть на входном изображении по горизонтали\n",
    "                Out[id_filter][x][y] = torch.sum(Input_padded[:, H_from:H_to, W_from:W_to] * W[id_filter]) # перемножаем матрицы Input_padded в нужной области и W поэлементно (у них должна быть одинаковая размерность) и складываем все элементы получившейся матрицы\n",
    "#====================================================================================================\n",
    "                Out[id_filter][x][y] += b[id_filter] # добавляем смещение фильтра (bias)\n",
    "                            \n",
    "    return Out # возвращаем полученный массив"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Реализация свёртки через im2col."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/im2col_conv.png\" alt=\"im2col convolution\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_im2col(Input, W, b, stride: int=1, padding: int=0, conv_back: bool=True) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Функция свёртки через im2col.\\n\n",
    "    Parameters:\n",
    "        * Input: входной массив размерности (Channels, Height, Width) ~ (Channels, H_in, W_in)\n",
    "        * W: веса для ядер свёртки\n",
    "        * b: bias, смещение для фильтра свёртки\n",
    "        * stride: размер шага по осям (одинаков как для оси OX, так и для OY)\n",
    "        * padding: значение количества добавляемх \"размерностей\" по краям входного массива, не включая размерность, отвечающую за каналы (одинаков как для оси OX, так и для OY)\n",
    "        * conv_back: флаг, нужно ли делать обратную конвертацию из 2d тензора (Num_filters, H_out*W_out) в 3d тензор (Num_filters, H_out, W_out)\\n\n",
    "    Returns:\n",
    "        * torch.Tensor: результат применения свёртки\n",
    "    \"\"\"\n",
    "    Channels, H_in, W_in = Input.shape # берём значения Channels, Height, Width из входной матрицы\n",
    "    Num_filters, Channels, H_conv, W_conv = W.shape # берём значения Num_filters, Channels, H_conv, W_conv из матрицы весов свёртки (Channels должен совпадать с аналогичной размерность из Input)\n",
    "\n",
    "    Input_padded = F.pad(input=Input, pad=(padding, padding, padding, padding, 0, 0), mode='constant', value=0) # добавляем padding из value на входной массив (0,0 — отвечают за добавляемую размерность в Channels (до и после исходных данных), остальные по аналогии за Width и Height)\n",
    "\n",
    "    H_out = int((H_in + 2*padding - (H_conv-1) - 1) / stride + 1) # Height выходного тензора\n",
    "    W_out = int((W_in + 2*padding - (W_conv-1) - 1) / stride + 1) # Width выходного тензора\n",
    "\n",
    "    # уменьшаем размерность матриц до 2d\n",
    "    Input_2d = torch.zeros(size=(H_conv*W_conv*Channels, H_out*W_out), dtype=torch.float32) # входной массив, преобразованный из 3d в 2d размерности (H_conv*W_conv*Channels, H_out*W_out)\n",
    "    for patch_id in range(H_out*W_out): # идём по столбцам 2d тензора входа (Patch_ам)\n",
    "        x = patch_id % W_out # номер рассматриваемой строки\n",
    "        H_from, H_to = stride*x, stride*x+H_conv # с какого пикселя по какой смотреть на входном изображении по вертикали\n",
    "        y = patch_id // W_out # номер рассматриваемого столбца\n",
    "        W_from, W_to = stride*y, stride*y+W_conv # с какого пикселя по какой смотреть на входном изображении по горизонтали\n",
    "        Input_2d[:, patch_id] = Input_padded[:, H_from:H_to, W_from:W_to].flatten() # записываем значение в колонку (flatten \"расправляет\" все элементы массива в вектор)\n",
    "    \n",
    "    W_2d = torch.zeros(size=(Num_filters, H_conv*W_conv*Channels), dtype=torch.float32) # фильтры, объединённые в двумерный тензор (из 3d в 2d) размерности (Num_filters, H_conv*W_conv*Channels)\n",
    "    for id_filter in range(Num_filters): # идём по строкам 2d тензора весов ядер (индексам фильтров в свёрточном слое)\n",
    "        W_2d[id_filter] = W[id_filter].flatten() # записываем значение в строку (flatten \"расправляет\" все элементы массива в вектор)\n",
    "\n",
    "    Out = W_2d @ Input_2d # перемножаем матрицы строку на столбец (@ — аналог torch.matmul)\n",
    "    for id_filter in range(Num_filters): # идём по строкам выходного тензора\n",
    "        Out[id_filter] += b[id_filter] # добавляем смещение фильтра (bias)\n",
    "\n",
    "    if conv_back: # если стоит флаг обратной конвертации\n",
    "        Out = Out.reshape(Num_filters, H_out, W_out) # приводим обратно к 3d тензору размерности (Num_filters, H_out, W_out)\n",
    "        Out = Out.transpose(1, 2) # транспонируем оси 1 и 2, так как тензор после reshape получается повёрнутым (0 ~ Num_filters ~ Channels не меняются, а Height и Width транспонируются)\n",
    "    return Out # возвращаем полученный массив"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Реализация Depthwise-separable свёртки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Тестирование"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Берём произвольное изображение (можно даже сгенерировать случайную матрицу) в формате torch.Tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 25, 25])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Input = tv.read_image('./data/bird_25x25.png') # считываем изображение; в Torch изображения идут сразу в формате RGB, но в виде (Channels, Height, Width)\n",
    "Input.shape # изначальная размерность изображения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пример визуализации изображения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "Input_show = Input.permute((1, 2, 0)) # приводим к формату, воспринимаемому в matplotlib, то есть меняем местами размерности: (Channels, Height, Width) => (Height, Width, Channels)\n",
    "Input_show.shape # размерность для matplotlib\n",
    "show_image(image=Input_show, figsize=(5, 5)) # вывод изображения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Конвертируем изображение в float32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[243., 241., 240.,  ..., 241., 241., 241.],\n",
      "         [242., 239., 238.,  ..., 239., 237., 237.],\n",
      "         [241., 240., 237.,  ..., 209., 233., 235.],\n",
      "         ...,\n",
      "         [207., 224., 215.,  ..., 221., 222., 228.],\n",
      "         [207., 179., 126.,  ..., 218., 221., 227.],\n",
      "         [136.,  99., 103.,  ..., 220., 222., 228.]],\n",
      "\n",
      "        [[215., 210., 211.,  ..., 225., 224., 225.],\n",
      "         [219., 213., 212.,  ..., 218., 215., 216.],\n",
      "         [221., 216., 212.,  ..., 194., 215., 220.],\n",
      "         ...,\n",
      "         [205., 227., 225.,  ..., 226., 226., 229.],\n",
      "         [208., 179., 119.,  ..., 223., 223., 225.],\n",
      "         [132.,  93.,  96.,  ..., 224., 224., 227.]],\n",
      "\n",
      "        [[212., 206., 203.,  ..., 226., 226., 227.],\n",
      "         [210., 205., 201.,  ..., 216., 215., 215.],\n",
      "         [210., 205., 201.,  ..., 195., 214., 217.],\n",
      "         ...,\n",
      "         [196., 232., 243.,  ..., 239., 239., 240.],\n",
      "         [212., 174.,  96.,  ..., 238., 238., 236.],\n",
      "         [108.,  67.,  67.,  ..., 238., 239., 238.]]])\n"
     ]
    }
   ],
   "source": [
    "Input = Input.to(torch.float32) # переводим изначальный tensor из int8 в float32\n",
    "print(Input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Вариант 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:04<00:00,  2.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Все тесты прошли успешно!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_conv(conv_simple, Input) # запускаем тестирование функции свёртки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Вариант 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:01<00:00,  1.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Все тесты прошли успешно!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_conv(conv_im2col, Input) # запускаем тестирование функции свёртки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Вариант 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Общее"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "stride = 2\n",
    "padding = 1\n",
    "Num_filters = 2\n",
    "\n",
    "Channels, H_in, W_in = 3, 5, 5\n",
    "Channels, H_conv, W_conv = 3, 3, 3\n",
    "\n",
    "# x = torch.ones(3, 5, 5)\n",
    "Input = torch.torch.randint(low=0, high=9, size=(Channels, H_in, W_in), dtype=torch.float32)\n",
    "Input_padded = F.pad(input=Input, pad=(padding, padding, padding, padding, 0, 0), mode='constant', value=0) # добавляем padding из value на входной массив (0,0 — отвечают за добавляемую размерность в Channels (до и после исходных данных), остальные по аналогии за Width и Height)\n",
    "\n",
    "H_out = int((H_in + 2*padding - (H_conv-1) - 1) / stride + 1) # Height выходного тензора\n",
    "W_out = int((W_in + 2*padding - (W_conv-1) - 1) / stride + 1) # Width выходного тензора\n",
    "\n",
    "# W = torch.ones(3, 5, 5)\n",
    "# W = torch.full((Channels, H_conv, W_conv), 2)\n",
    "W = torch.randint(low=0, high=10, size=(Num_filters, Channels, H_conv, W_conv), dtype=torch.float32)\n",
    "\n",
    "b = torch.randint(low=0, high=10, size=(Num_filters,), dtype=torch.float32) # bias по аналогии"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 4., 5., 1., 2., 0.],\n",
       "         [0., 8., 4., 6., 8., 1., 0.],\n",
       "         [0., 5., 2., 3., 7., 0., 0.],\n",
       "         [0., 6., 5., 4., 5., 1., 0.],\n",
       "         [0., 3., 0., 3., 6., 3., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 1., 0., 3., 4., 8., 0.],\n",
       "         [0., 5., 4., 2., 2., 5., 0.],\n",
       "         [0., 5., 2., 2., 1., 5., 0.],\n",
       "         [0., 5., 2., 1., 3., 4., 0.],\n",
       "         [0., 7., 7., 5., 8., 4., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 4., 2., 8., 8., 4., 0.],\n",
       "         [0., 3., 5., 7., 3., 5., 0.],\n",
       "         [0., 6., 5., 0., 3., 7., 0.],\n",
       "         [0., 7., 3., 0., 1., 3., 0.],\n",
       "         [0., 8., 8., 3., 2., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Input_padded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "Input = tv.read_image('./data/bird_256x256.png') # считываем изображение; в Torch изображения идут сразу в формате RGB, но в виде (Channels, Height, Width)\n",
    "Input = Input.to(torch.float32) # переводим int8 в float32\n",
    "\n",
    "# W = torch.ones((2, 3, 3, 3), dtype=torch.float32) # веса для ядер свёртки\n",
    "# b = torch.ones((2), dtype=torch.float32) # bias\n",
    "\n",
    "# W = torch.rand((2, 3, 3, 3), dtype=torch.float32) # веса для ядер свёртки\n",
    "# b = torch.rand((2), dtype=torch.float32) # bias\n",
    "\n",
    "W = torch.randint(low=0, high=10, size=(2, 3, 3, 3), dtype=torch.float32) # веса для ядер свёртки\n",
    "b = torch.randint(low=0, high=10, size=(2,), dtype=torch.float32) # bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = conv_im2col(Input, W, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[28101., 28205., 28310.,  ..., 29422., 29397., 29384.],\n",
       "        [20621., 20693., 20786.,  ..., 21809., 21789., 21779.]])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = torch.nn.Conv2d(Channels, Num_filters, kernel_size=3, stride=stride, padding=padding)\n",
    "m.bias.data = b\n",
    "m.weight.data = W\n",
    "\n",
    "res_ = m(Input.to(torch.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.equal(res_, res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[12733., 19586., 19620.,  ..., 20875., 20831., 20874.],\n",
       "         [19474., 28173., 28183.,  ..., 29747., 29688., 29750.],\n",
       "         [19574., 28400., 28373.,  ..., 29554., 29444., 29420.],\n",
       "         ...,\n",
       "         [ 8347., 11862., 11936.,  ..., 29458., 29450., 29466.],\n",
       "         [ 8221., 12232., 12514.,  ..., 29427., 29412., 29422.],\n",
       "         [ 8694., 12481., 12107.,  ..., 29434., 29404., 29384.]],\n",
       "\n",
       "        [[ 9721., 12239., 12268.,  ..., 13067., 13039., 13072.],\n",
       "         [14904., 20660., 20659.,  ..., 21926., 21877., 21918.],\n",
       "         [14997., 20841., 20819.,  ..., 21801., 21711., 21698.],\n",
       "         ...,\n",
       "         [ 6313.,  8599.,  8610.,  ..., 21834., 21819., 21833.],\n",
       "         [ 6282.,  8838.,  9031.,  ..., 21818., 21803., 21809.],\n",
       "         [ 6673.,  9006.,  8821.,  ..., 21827., 21790., 21779.]]],\n",
       "       grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[494., 487., 441., 487., 511., 402., 471., 358., 398.],\n",
       "        [479., 500., 451., 513., 427., 362., 466., 307., 344.],\n",
       "        [517., 516., 409., 458., 446., 420., 451., 397., 431.],\n",
       "        [502., 538., 529., 442., 502., 387., 401., 389., 341.],\n",
       "        [571., 510., 530., 528., 485., 388., 520., 362., 440.]])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
