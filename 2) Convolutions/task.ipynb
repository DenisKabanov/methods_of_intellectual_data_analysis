{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Реализовать стандартную функцию свертки​.\n",
    "2) Реализовать функцию свертки через im2col.\n",
    "3) Реализовать Depthwise-separable свертку.\n",
    "\n",
    "* Специфицировать размер, количество фильтров, входной tensor, stride, padding...\n",
    "* Придумать тесты для проверки работоспособности вариантов свёртки.​"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Настройки/Импорты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\User\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # для работы с массивами\n",
    "import torch # для работы с тензорами\n",
    "import torch.nn.functional as F # для функций с тензорами\n",
    "import torchvision.io as tv # для работы с изображениями\n",
    "import keras # для проверки Depthwise-separable свёртки\n",
    "from tqdm import tqdm # для отслеживания прогресса\n",
    "from matplotlib import pyplot as plt # для построения графиков/вывода изображений"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Вспомогательные функции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image(image, figsize: tuple, gray: bool=False) -> None:\n",
    "    \"\"\"\n",
    "    Функция для вывода изображений.\\n\n",
    "    Parameters:\n",
    "        * image: изображение, что нужно вывести\n",
    "        * figsize: размер выводимой картинки\n",
    "        * gray: флаг, является ли изображение одноканальным (чёрно-белым)\\n\n",
    "    Returns:\n",
    "        * None\n",
    "    \"\"\"\n",
    "    cmap = \"gray\" if gray else \"viridis\" # выбор цветовой схемы, (стандартная — \"viridis\", но если передан ключ gray — \"gray\")\n",
    "\n",
    "    plt.figure(figsize=figsize) # размер фигуры\n",
    "    plt.imshow(image, cmap) # вывод изображения\n",
    "    plt.show() # показ фигуры"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Реализация стандартной свёртки."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/conv_simple.png\" alt=\"Simple convolution\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция свёртки:\n",
    "$$\n",
    "\\mathbf{Out}[id_{conv}][x][y] = \\sum_{i=0}^{H_{conv}-1}\\sum_{j=0}^{W_{conv}-1}\\sum_{c=0}^{Channels-1}\\mathbf{Input}[c][Sx+i][Sy+j] * \\mathbf{W}[id_{conv}][c][i][j] + \\mathbf{b}[id_{conv}]\n",
    "$$\n",
    "Где:\n",
    "* Input — входная матрица размерности (Channels, Height, Width) ~ (Channels, $H_{in}$, $W_{in}$)\n",
    "* W — матрица весов свёрточного слоя размерности (Num_filters, Channels, $H_{conv}$, $W_{conv}$)\n",
    "* b — смещения (bias) для вильтров свёртки (одно число на фильтр) размерности (Num_filters)\n",
    "* Out — выходная матрица (Num_filters, $H_{out}$, $W_{out}$), где $H_{out}$, $W_{out}$ зависят от параметров свёртки\n",
    "* $id_{conv}$  — номер рассматриваемой свёртки (фильтра свёртки), изменяется от 0 до Num_filters-1\n",
    "* x — координата **нового** пикселя по вертикали ~ номер строки (в изначальном изображении — Height ~ $H_{in}$)\n",
    "* y — координата **нового** пикселя по горизонтали ~ номер столбца (в изначальном изображении — Width ~ $W_{in}$)\n",
    "* $H_{conv}$ — размер ядра свёртки по вертикали (итерируемся с помощью *i*)\n",
    "* $W_{conv}$ — размер ядра свёртки по горизонтали (итерируемся с помощью *j*)\n",
    "* Channels — число каналов (фильтров) у входного тензора (итерируемся с помощью *c*)\n",
    "* S — значение параметра \"Stride\", отвечающее за размер шага по изображению (смещение по изображению)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_simple(Input, W, b, stride: int=1, padding: int=0) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Функция простой свёртки.\\n\n",
    "    Parameters:\n",
    "        * Input: входной массив размерности (Channels, Height, Width) ~ (Channels, H_in, W_in)\n",
    "        * W: веса для ядер свёртки\n",
    "        * b: bias, смещение для фильтра свёртки\n",
    "        * stride: размер шага по осям (одинаков как для оси OX, так и для OY)\n",
    "        * padding: значение количества добавляемх \"размерностей\" по краям входного массива, не включая размерность, отвечающую за каналы (одинаков как для оси OX, так и для OY)\\n\n",
    "    Returns:\n",
    "        * torch.Tensor: результат применения свёртки\n",
    "    \"\"\"\n",
    "    Channels, H_in, W_in = Input.shape # берём значения Channels, Height, Width из входной матрицы\n",
    "    Num_filters, Channels, H_conv, W_conv = W.shape # берём значения Num_filters, Channels, H_conv, W_conv из матрицы весов свёртки (Channels должен совпадать с аналогичной размерность из Input)\n",
    "\n",
    "    Input_padded = F.pad(input=Input, pad=(padding, padding, padding, padding, 0, 0), mode='constant', value=0) # добавляем padding из value на входной массив (0,0 — отвечают за добавляемую размерность в Channels (до и после исходных данных), остальные по аналогии за Width и Height)\n",
    "\n",
    "    H_out = int((H_in + 2*padding - (H_conv-1) - 1) / stride + 1) # Height выходного тензора\n",
    "    W_out = int((W_in + 2*padding - (W_conv-1) - 1) / stride + 1) # Width выходного тензора\n",
    "    Out = torch.zeros(size=(Num_filters, H_out, W_out), dtype=torch.float32) # заготовка под выходной тензор, заполненная нулями\n",
    "    \n",
    "    # print(f\"Input_padded shape: {Input_padded.shape}\")\n",
    "    # print(f\"Out shape: {Out.shape}\")\n",
    "\n",
    "    for id_filter in range(Num_filters): # идём по числу фильтров в одной свёртке (range вернёт список чисел от 0 до Num_filters-1 включительно)\n",
    "        for x in range(H_out): # идём по пикселям высоты выходного тензора\n",
    "            for y in range(W_out): # идём по пикселям ширины выходного тензора\n",
    "#=========================== вариант через медленное сложение всех элементов ========================\n",
    "                # for i in range(H_conv): # идём по высоте фильтра (ядра) свёртки\n",
    "                #     for j in range(W_conv): # идём по ширине фильтра (ядра) свёртки\n",
    "                #         for c in range(Channels): # идём по каналам входного изображения\n",
    "                #             Out[id_filter][x][y] += Input_padded[c][stride*x + i][stride*y + j] * W [id_filter][c][i][j]\n",
    "#--------------------------- вариант через быстрое перемножение тензоров ----------------------------\n",
    "                H_from, H_to = stride*x, stride*x+H_conv # с какого пикселя по какой смотреть на входном изображении по вертикали\n",
    "                W_from, W_to = stride*y, stride*y+W_conv # с какого пикселя по какой смотреть на входном изображении по горизонтали\n",
    "                Out[id_filter][x][y] = torch.sum(Input_padded[:, H_from:H_to, W_from:W_to] * W[id_filter]) # перемножаем матрицы Input_padded в нужной области и W поэлементно (у них должна быть одинаковая размерность) и складываем все элементы получившейся матрицы\n",
    "#====================================================================================================\n",
    "                Out[id_filter][x][y] += b[id_filter] # добавляем смещение фильтра (bias)\n",
    "                            \n",
    "    return Out # возвращаем полученный массив"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Реализация свёртки через im2col."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/conv_im2col.png\" alt=\"im2col convolution\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_im2col(Input, W, b, stride: int=1, padding: int=0, conv_back: bool=True) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Функция свёртки через im2col.\\n\n",
    "    Parameters:\n",
    "        * Input: входной массив размерности (Channels, Height, Width) ~ (Channels, H_in, W_in)\n",
    "        * W: веса для ядер свёртки\n",
    "        * b: bias, смещение для фильтра свёртки\n",
    "        * stride: размер шага по осям (одинаков как для оси OX, так и для OY)\n",
    "        * padding: значение количества добавляемх \"размерностей\" по краям входного массива, не включая размерность, отвечающую за каналы (одинаков как для оси OX, так и для OY)\n",
    "        * conv_back: флаг, нужно ли делать обратную конвертацию из 2d тензора (Num_filters, H_out*W_out) в 3d тензор (Num_filters, H_out, W_out)\\n\n",
    "    Returns:\n",
    "        * torch.Tensor: результат применения свёртки\n",
    "    \"\"\"\n",
    "    Channels, H_in, W_in = Input.shape # берём значения Channels, Height, Width из входной матрицы\n",
    "    Num_filters, Channels, H_conv, W_conv = W.shape # берём значения Num_filters, Channels, H_conv, W_conv из матрицы весов свёртки (Channels должен совпадать с аналогичной размерность из Input)\n",
    "\n",
    "    Input_padded = F.pad(input=Input, pad=(padding, padding, padding, padding, 0, 0), mode='constant', value=0) # добавляем padding из value на входной массив (0,0 — отвечают за добавляемую размерность в Channels (до и после исходных данных), остальные по аналогии за Width и Height)\n",
    "\n",
    "    H_out = int((H_in + 2*padding - (H_conv-1) - 1) / stride + 1) # Height выходного тензора\n",
    "    W_out = int((W_in + 2*padding - (W_conv-1) - 1) / stride + 1) # Width выходного тензора\n",
    "\n",
    "    # уменьшаем размерность матриц до 2d\n",
    "    Input_2d = torch.zeros(size=(H_conv*W_conv*Channels, H_out*W_out), dtype=torch.float32) # входной массив, преобразованный из 3d в 2d размерности (H_conv*W_conv*Channels, H_out*W_out)\n",
    "    for id_patch in range(H_out*W_out): # идём по столбцам 2d тензора входа (Patch_ам)\n",
    "        x = id_patch % W_out # номер рассматриваемой строки\n",
    "        H_from, H_to = stride*x, stride*x+H_conv # с какого пикселя по какой смотреть на входном изображении по вертикали\n",
    "        y = id_patch // W_out # номер рассматриваемого столбца\n",
    "        W_from, W_to = stride*y, stride*y+W_conv # с какого пикселя по какой смотреть на входном изображении по горизонтали\n",
    "        Input_2d[:, id_patch] = Input_padded[:, H_from:H_to, W_from:W_to].flatten() # записываем значение в колонку (flatten \"расправляет\" все элементы массива в вектор)\n",
    "    \n",
    "    W_2d = torch.zeros(size=(Num_filters, H_conv*W_conv*Channels), dtype=torch.float32) # фильтры, объединённые в двумерный тензор (из 3d в 2d) размерности (Num_filters, H_conv*W_conv*Channels)\n",
    "    for id_filter in range(Num_filters): # идём по строкам 2d тензора весов ядер (индексам фильтров в свёрточном слое)\n",
    "        W_2d[id_filter] = W[id_filter].flatten() # записываем значение в строку (flatten \"расправляет\" все элементы массива в вектор)\n",
    "\n",
    "    Out = W_2d @ Input_2d # перемножаем матрицы строку на столбец (@ — аналог torch.matmul)\n",
    "    for id_filter in range(Num_filters): # идём по строкам выходного тензора\n",
    "        Out[id_filter] += b[id_filter] # добавляем смещение фильтра (bias)\n",
    "\n",
    "    if conv_back: # если стоит флаг обратной конвертации\n",
    "        Out = Out.reshape(Num_filters, H_out, W_out) # приводим обратно к 3d тензору размерности (Num_filters, H_out, W_out)\n",
    "        Out = Out.transpose(1, 2) # транспонируем оси 1 и 2, так как тензор после reshape получается повёрнутым (0 ~ Num_filters ~ Channels не меняются, а Height и Width транспонируются)\n",
    "    return Out # возвращаем полученный массив"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Реализация Depthwise-separable свёртки."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/conv_depthwise_1.png\" alt=\"Depthwise-separable convolution\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/conv_depthwise_2.png\" alt=\"Depthwise-separable convolution\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_depthwise_separable(Input, W_depthwise, W_pointwise, b_pointwise, stride: int=1, padding: int=0) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Функция Depthwise-separable свёртки.\\n\n",
    "    Parameters:\n",
    "        * Input: входной массив размерности (Channels, Height, Width) ~ (Channels, H_in, W_in)\n",
    "        * W_depthwise: веса для ядер Depth-wise свёртки\n",
    "        * W_pointwise: веса для ядер Point-wise свёртки\n",
    "        * b_pointwise: bias, смещение для фильтра свёртки (добавляется после Point-wise свёртки)\n",
    "        * stride: размер шага по осям (одинаков как для оси OX, так и для OY)\n",
    "        * padding: значение количества добавляемх \"размерностей\" по краям входного массива, не включая размерность, отвечающую за каналы (одинаков как для оси OX, так и для OY)\\n\n",
    "    Returns:\n",
    "        * torch.Tensor: результат применения свёртки\n",
    "    \"\"\"\n",
    "    Channels, H_in, W_in = Input.shape # берём значения (Channels, Height, Width) из входной матрицы\n",
    "    Channels, H_conv, W_conv = W_depthwise.shape # берём значения (Channels, H_conv, W_conv) из матрицы весов Depth-wise свёртки (Channels должен совпадать с аналогичной размерность из Input)\n",
    "    Num_filters, Channels, _, _ = W_pointwise.shape # берём значения (Num_filters, Channels, 1, 1) из матрицы весов Point-wise свёртки (Channels должен совпадать с аналогичной размерность из Input)\n",
    "\n",
    "    Input_padded = F.pad(input=Input, pad=(padding, padding, padding, padding, 0, 0), mode='constant', value=0) # добавляем padding из value на входной массив (0,0 — отвечают за добавляемую размерность в Channels (до и после исходных данных), остальные по аналогии за Width и Height)\n",
    "\n",
    "    H_out = int((H_in + 2*padding - (H_conv-1) - 1) / stride + 1) # Height выходного тензора\n",
    "    W_out = int((W_in + 2*padding - (W_conv-1) - 1) / stride + 1) # Width выходного тензора\n",
    "\n",
    "    # Depth-wise convolution\n",
    "    Out_depthwise = torch.zeros(size=(Channels, H_out, W_out), dtype=torch.float32) # заготовка под выходной тензор с Depthwise шага, заполненная нулями\n",
    "    for id_channel in range(Channels): # идём по каналам\n",
    "        for x in range(H_out): # идём по пикселям высоты выходного тензора\n",
    "            for y in range(W_out): # идём по пикселям ширины выходного тензора\n",
    "                H_from, H_to = stride*x, stride*x+H_conv # с какого пикселя по какой смотреть на входном изображении по вертикали\n",
    "                W_from, W_to = stride*y, stride*y+W_conv # с какого пикселя по какой смотреть на входном изображении по горизонтали\n",
    "                Out_depthwise[id_channel][x][y] = torch.sum(Input_padded[id_channel, H_from:H_to, W_from:W_to] * W_depthwise[id_channel]) # перемножаем матрицы Input_padded в нужной области и W_depthwise поэлементно (у них должна быть одинаковая размерность) и складываем все элементы получившейся матрицы\n",
    "    \n",
    "    # Point-wise convolution\n",
    "    Out_pointwise = torch.zeros(size=(Num_filters, H_out, W_out), dtype=torch.float32) # заготовка под выходной тензор с Pointwise шага, заполненная нулями\n",
    "    for id_filter in range(Num_filters): # идём по числу фильтров в одной свёртке (range вернёт список чисел от 0 до Num_filters-1 включительно)\n",
    "        Out_pointwise[id_filter] = torch.sum(W_pointwise[id_filter] * Out_depthwise, dim=0) # умножаем свёртку с ядром 1x1, отвечающую за фильтр id_filter на пришедший массив с шага Depthwise (делаем так по числу необходимых фильтров), а затем суммируем получившиеся значения на каналах\n",
    "        # каналы (Channels) тензора с шага Depthwise свёртки умножаются на соответствующие им коэффициенты из W_pointwise, таким образом каждый канал ПОЛНОСТЬЮ умножается на определённое число, после чего все каналы просуммируются (останется тензор размерности (H_out, W_out))\n",
    "        # dim=0 — суммирование по Channels (чтобы потом эта размерность занулилась и заменилась на число фильтров — Num_filters)\n",
    "        Out_pointwise[id_filter] += b_pointwise[id_filter] # добавляем смещение фильтра (bias)\n",
    "    \n",
    "    return Out_pointwise # возвращаем полученный массив"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Тестирование"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Берём произвольное изображение (можно даже сгенерировать случайную матрицу) в формате torch.Tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 25, 25])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Input = tv.read_image('./data/bird_25x25.png') # считываем изображение; в Torch изображения идут сразу в формате RGB, но в виде (Channels, Height, Width)\n",
    "Input.shape # изначальная размерность изображения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пример визуализации изображения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "Input_show = Input.permute((1, 2, 0)) # приводим к формату, воспринимаемому в matplotlib, то есть меняем местами размерности: (Channels, Height, Width) => (Height, Width, Channels)\n",
    "Input_show.shape # размерность для matplotlib\n",
    "show_image(image=Input_show, figsize=(5, 5)) # вывод изображения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Конвертируем изображение в float32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[243., 241., 240.,  ..., 241., 241., 241.],\n",
      "         [242., 239., 238.,  ..., 239., 237., 237.],\n",
      "         [241., 240., 237.,  ..., 209., 233., 235.],\n",
      "         ...,\n",
      "         [207., 224., 215.,  ..., 221., 222., 228.],\n",
      "         [207., 179., 126.,  ..., 218., 221., 227.],\n",
      "         [136.,  99., 103.,  ..., 220., 222., 228.]],\n",
      "\n",
      "        [[215., 210., 211.,  ..., 225., 224., 225.],\n",
      "         [219., 213., 212.,  ..., 218., 215., 216.],\n",
      "         [221., 216., 212.,  ..., 194., 215., 220.],\n",
      "         ...,\n",
      "         [205., 227., 225.,  ..., 226., 226., 229.],\n",
      "         [208., 179., 119.,  ..., 223., 223., 225.],\n",
      "         [132.,  93.,  96.,  ..., 224., 224., 227.]],\n",
      "\n",
      "        [[212., 206., 203.,  ..., 226., 226., 227.],\n",
      "         [210., 205., 201.,  ..., 216., 215., 215.],\n",
      "         [210., 205., 201.,  ..., 195., 214., 217.],\n",
      "         ...,\n",
      "         [196., 232., 243.,  ..., 239., 239., 240.],\n",
      "         [212., 174.,  96.,  ..., 238., 238., 236.],\n",
      "         [108.,  67.,  67.,  ..., 238., 239., 238.]]])\n"
     ]
    }
   ],
   "source": [
    "Input = Input.to(torch.float32) # переводим изначальный tensor из int8 в float32\n",
    "print(Input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция для проверки работоспособности свёрток при различных параметрах (применится для стандартной свёртки и im2col):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_conv(func, Input) -> None:\n",
    "    \"\"\"\n",
    "    Функция для тестирования работы свёрток. Сравнивает с реализацией свёрточного слоя на PyTorch при различных параметрах.\\n\n",
    "    Parameters:\n",
    "        * func: функция свёртки, что нужно протестировать\n",
    "        * Input: входной массив, на котором будет проводиться тестирование (в формате float32) размерности (Channels, Height, Width) ~ (Channels, H_in, W_in)\\n \n",
    "    Returns:\n",
    "        * None: выдаст Exception в случае, если есть несовпадение ответов у проверяемой и контрольной функции свёртки\n",
    "    \"\"\"\n",
    "    Channels, H_in, W_in = Input.shape # берём значения Channels, Height, Width из входной матрицы\n",
    "\n",
    "    for Num_filters in tqdm([1, 2]): # рассматриваем возможное число фильтров\n",
    "        for kernel_size in [1, 2, 3, 5, 7]: # рассматриваем возможные размеры ядер\n",
    "\n",
    "            W = torch.randint(low=0, high=10, size=(Num_filters, Channels, kernel_size, kernel_size), dtype=torch.float32) # случайно генерируем веса для ядер свёртки (int от 0 до 10 — чтобы решение всегда было точным с последующей конвертацией в float32)\n",
    "            # размерность W: (число ядер(фильтров) на слое свёртки, Channels — число входных каналов, высота ядра свёртки H_conv, ширина ядра свёртки W_conv)\n",
    "            b = torch.randint(low=0, high=10, size=(Num_filters,), dtype=torch.float32) # bias по аналогии\n",
    "            # размерность b: (число ядер(фильтров) на слое свёртки)\n",
    "\n",
    "            for stride in [1, 2, 5]: # рассматриваем варианты параметра stride\n",
    "                for padding in [0, 1, 2]: # рассматриваем варианты параметра padding\n",
    "                    # написанная функция\n",
    "                    res_func = func(Input, W, b, stride, padding) # считаем результат проверяемой свёртки\n",
    "                    \n",
    "                    # реализация свёртки на PyTorch\n",
    "                    conv_torch = torch.nn.Conv2d(in_channels=Channels, out_channels=Num_filters, kernel_size=kernel_size, stride=stride, padding=padding) # слой свёртки из PyTorch\n",
    "                    conv_torch.weight.data = W # меняем веса ядер свёртки на тестируемые (со случайных, заданных при создании Conv2d)\n",
    "                    conv_torch.bias.data = b # меняем bias ядер свёртки на тестируемые (со случайных, заданных при создании Conv2d)\n",
    "                    \n",
    "                    res_torch = conv_torch(Input) # считаем результат контрольной свёртки\n",
    "                    \n",
    "\n",
    "                    if not torch.equal(res_func, res_torch): # сравниваем, что все значения в тензорах одинаковые\n",
    "                        raise Exception(f\"Результат свёрток не совпал при параметрах: Num_filters={Num_filters}, kernel_size={kernel_size}, stride={stride}, padding={padding}\") # если есть несовпадения — выкидываем исключение\n",
    "    print(\"Все тесты прошли успешно!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Вариант 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:04<00:00,  2.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Все тесты прошли успешно!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_conv(conv_simple, Input) # запускаем тестирование функции простой свёртки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Вариант 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:01<00:00,  1.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Все тесты прошли успешно!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_conv(conv_im2col, Input) # запускаем тестирование функции свёртки через im2col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Вариант 3:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сравним работу Depthwise-separable свёртки с аналогичной реализацией на PyTorch и Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeparableConv2d(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Класс, реализующий Depthwise-separable свёртку на PyTorch.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, padding): # конструктор объекта класса\n",
    "        super(SeparableConv2d, self).__init__() # для правильной инициализации базового класса torch.nn.Module\n",
    "        self.depthwise = torch.nn.Conv2d(in_channels=in_channels, out_channels=in_channels, kernel_size=kernel_size, stride=stride, padding=padding, groups=in_channels, bias=False)\n",
    "        # свёртка Depthwise шага, веса имеют размерность (Channels, 1, H_conv, W_conv)\n",
    "        # выход слоя размерности (Channels, H_out, W_out), где H_out и W_out считаются по особой формуле\n",
    "        # stride — размер шага по входному массиву, padding — размер добавляемых отступов для осей Height и Width входного массива\n",
    "        # groups=in_channels означает, что каждому входному каналу будет соответствовать только один свёрточный слой!\n",
    "        # bias=False — данный слой не имеет смещения\n",
    "\n",
    "        self.pointwise = torch.nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=(1), stride=1, padding=0, groups=1, bias=True)\n",
    "        # свёртка Pointwise шага, веса имеют размерность (Num_filters, Channels, 1, 1)\n",
    "        # выход слоя размерности (Num_filters, H_out, W_out), где H_out и W_out считаются по особой формуле\n",
    "        # stride — размер шага по входному массиву (1 — идём без пропусков), padding — размер добавляемых отступов для осей Height и Width входного массива (0 — не добавляется)\n",
    "        # groups=1 означает, что каждому входному канал будет обрабатываться всеми фильтрами (ядрами) свёртки\n",
    "        # bias=True — данный слой имеет смещение\n",
    "\n",
    "    def forward(self, x): # функция, вызываемая оператором () у объекта класса\n",
    "        out = self.depthwise(x) # Depthwise шаг\n",
    "        out = self.pointwise(out) # Pointwise шаг\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_depthwise_conv(Input) -> None:\n",
    "    \"\"\"\n",
    "    Функция для тестирования работы Depthwise-separable свёртки. Сравнивает с аналогичной реализацией свёрточного слоя на PyTorch и Keras при различных параметрах.\\n\n",
    "    Parameters:\n",
    "        * Input: входной массив, на котором будет проводиться тестирование (в формате float32) размерности (Channels, Height, Width) ~ (Channels, H_in, W_in)\\n \n",
    "    Returns:\n",
    "        * None: выдаст Exception в случае, если есть несовпадение ответов у проверяемой и контрольной функции свёртки\n",
    "    \"\"\"\n",
    "    Channels, H_in, W_in = Input.shape # берём значения Channels, Height, Width из входной матрицы\n",
    "\n",
    "    for Num_filters in tqdm([1, 2]): # рассматриваем возможное число фильтров\n",
    "        for kernel_size in [1, 2, 3, 5, 7]: # рассматриваем возможные размеры ядер\n",
    "            H_conv, W_conv = kernel_size, kernel_size # размер ядра (фильтра) свёртки\n",
    "\n",
    "            W_depthwise = torch.randint(low=0, high=10, size=(Channels, H_conv, W_conv), dtype=torch.float32) # случайно генерируем веса для ядер Depthwise свёртки (int от 0 до 10 — чтобы решение всегда было точным с последующей конвертацией в float32)\n",
    "            # размерность W_depthwise: (Channels — число входных каналов, высота ядра свёртки H_conv, ширина ядра свёртки W_conv)\n",
    "            W_pointwise = torch.randint(low=0, high=10, size=(Num_filters, Channels, 1, 1), dtype=torch.float32) # случайно генерируем веса для ядер Pointwise свёртки (int от 0 до 10 — чтобы решение всегда было точным с последующей конвертацией в float32)\n",
    "            # размерность W_pointwise: (число ядер(фильтров) на слое свёртки, Channels — число входных каналов, 1 — высота ядра, 1 — ширина ядра)\n",
    "            b_pointwise = torch.randint(low=0, high=10, size=(Num_filters,), dtype=torch.float32) # bias, применяемый после Pointwise свёртки\n",
    "            # размерность b_pointwise: (число ядер(фильтров) на слое свёртки)\n",
    "\n",
    "            W_depthwise_torch = W_depthwise.reshape(Channels, 1, H_conv, W_conv) # конвертируем веса Depthwise свёртки для понимания PyTorch моделью, добавляя промежуточную размерность (чтобы в PyTorch каждому каналу соответствовала одна свёртка)\n",
    "            W_depthwise_keras = W_depthwise_torch.permute(2, 3, 0, 1) # конвертируем веса Depthwise свёртки для понимания Keras моделью (ПРАВИЛЬНО меняем размерности весов PyTorch местами)\n",
    "\n",
    "            W_pointwise_torch = W_pointwise # в PyTorch веса для Pointwise свёртки не меняются\n",
    "            W_pointwise_keras = W_pointwise.permute(3, 2, 1, 0) # конвертируем веса Pointwise свёртки для понимания Keras моделью (полностью поворачиваем размерности)\n",
    "\n",
    "            b_pointwise_torch = b_pointwise # смещение не меняется\n",
    "            b_pointwise_keras = b_pointwise # смещение не меняется\n",
    "\n",
    "\n",
    "            for stride in [1, 2, 5]: # рассматриваем варианты параметра stride\n",
    "                for padding in [0, 1, 2]: # рассматриваем варианты параметра padding\n",
    "                    # написанная функция\n",
    "                    res_func = conv_depthwise_separable(Input, W_depthwise, W_pointwise, b_pointwise, stride, padding) # считаем результат проверяемой свёртки\n",
    "\n",
    "                    # реализация свёртки на PyTorch\n",
    "                    conv_torch = SeparableConv2d(in_channels=Channels, out_channels=Num_filters, kernel_size=(H_conv, W_conv), stride=stride, padding=padding) # Depthwise-separable свёртка на PyTorch\n",
    "                    conv_torch.depthwise.weight.data = W_depthwise_torch # устанавливаем веса для Depthwise свёртки\n",
    "                    conv_torch.pointwise.weight.data = W_pointwise_torch # устанавливаем веса для Pointwise свёртки\n",
    "                    conv_torch.pointwise.bias.data = b_pointwise_torch # устанавливаем смещение для Pointwise свёртки\n",
    "                    \n",
    "                    res_torch = conv_torch(Input) # считаем результат свёртки на PyTorch\n",
    "                    if not torch.equal(res_func, res_torch): # сравниваем, что все значения в тензорах одинаковые\n",
    "                        raise Exception(f\"Результат свёртки не совпал с PyTorch при параметрах: Num_filters={Num_filters}, kernel_size={kernel_size}, stride={stride}, padding={padding}\") # если есть несовпадения — выкидываем исключение\n",
    "\n",
    "                    # реализация свёртки на Keras\n",
    "                    Input_padded = keras.layers.ZeroPadding2D(padding=(padding, padding), data_format='channels_first')(np.array([Input])) # добавляем padding для массива (теперь он станет не torch.tensor, а np.array) на оси, отвечающие за Height и Width, а также с помощью дополнительных скобок \"[Input]\" имитируем batch из одного элемента, необходимый для работы Keras\n",
    "                    conv_keras = keras.layers.SeparableConv2D(filters=Num_filters, kernel_size=(H_conv, W_conv), strides=stride, padding=\"valid\", activation=None, data_format='channels_first') # Depthwise-separable свёртка на Лукфы\n",
    "                    _ = conv_keras(Input_padded) # делаем фиктивный запуск для инициализации весов свёртки в keras \n",
    "                    conv_keras.set_weights([W_depthwise_keras, W_pointwise_keras, b_pointwise_keras]) # заменяем случайно инициализированные веса на те, что использовались до этого\n",
    "\n",
    "                    res_keras = conv_keras(Input_padded) # считаем результат свёртки на Keras\n",
    "                    if not np.all(res_func == res_keras): # сравниваем, что все значения в torch.Tensor и np.array одинаковые (через np.all, так как он игнорируем привязку градиентов и различие типов torch.Tensor и np.array)\n",
    "                        raise Exception(f\"Результат свёртки не совпал с Keras при параметрах: Num_filters={Num_filters}, kernel_size={kernel_size}, stride={stride}, padding={padding}\") # если есть несовпадения — выкидываем исключение\n",
    "    print(\"Все тесты прошли успешно!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\User\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:09<00:00,  4.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Все тесты прошли успешно!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_depthwise_conv(Input) # запускаем тестирование функции Depthwise-separable свёртки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Пример работы с произвольным тензором"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ожидаемая размерность выходного тензора: (Num_filters: 6, Height: 2, Width:2).\n"
     ]
    }
   ],
   "source": [
    "stride = 2 # размер шага по осям (одинаков как для оси OX, так и для OY)\n",
    "padding = 1 # размер добавляемых отступов для осей Height и Width входного массива\n",
    "\n",
    "Channels, H_in, W_in = 3, 5, 5 # число каналов, высота и ширина у входного тензора\n",
    "H_conv, W_conv = 4, 4 # размеры ядер (фильтров) свёрток\n",
    "Num_filters = 6 # число каналов (фильтров) на выходе\n",
    "\n",
    "Input = torch.randint(low=0, high=9, size=(Channels, H_in, W_in), dtype=torch.float32) # случайно генерируем тензор\n",
    "Input_padded = F.pad(input=Input, pad=(padding, padding, padding, padding, 0, 0), mode='constant', value=0) # добавляем padding из value на входной массив (0,0 — отвечают за добавляемую размерность в Channels (до и после исходных данных), остальные по аналогии за Width и Height)\n",
    "\n",
    "H_out = int((H_in + 2*padding - (H_conv-1) - 1) / stride + 1) # Height выходного тензора\n",
    "W_out = int((W_in + 2*padding - (W_conv-1) - 1) / stride + 1) # Width выходного тензора\n",
    "print(f\"Ожидаемая размерность выходного тензора: (Num_filters: {Num_filters}, Height: {H_out}, Width:{W_out}).\")\n",
    "\n",
    "W = torch.randint(low=0, high=10, size=(Num_filters, Channels, H_conv, W_conv), dtype=torch.float32) # случайно генерируем веса для ядер свёртки (int от 0 до 10 — чтобы решение всегда было точным с последующей конвертацией в float32)\n",
    "b = torch.randint(low=0, high=10, size=(Num_filters,), dtype=torch.float32) # bias по аналогии\n",
    "# W_depthwise = torch.randint(low=0, high=10, size=(Channels, H_conv, W_conv), dtype=torch.float32) # случайно генерируем веса для ядер Depthwise свёртки (int от 0 до 10 — чтобы решение всегда было точным с последующей конвертацией в float32)\n",
    "# W_pointwise = torch.randint(low=0, high=10, size=(Num_filters, Channels, 1, 1), dtype=torch.float32) # случайно генерируем веса для ядер Pointwise свёртки (int от 0 до 10 — чтобы решение всегда было точным с последующей конвертацией в float32)\n",
    "# b_pointwise = torch.randint(low=0, high=10, size=(Num_filters,), dtype=torch.float32) # bias, применяемый после Pointwise свёртки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 5., 3., 7., 3., 0., 0.],\n",
       "         [0., 0., 5., 2., 1., 5., 0.],\n",
       "         [0., 4., 6., 3., 2., 1., 0.],\n",
       "         [0., 6., 8., 5., 6., 1., 0.],\n",
       "         [0., 6., 4., 3., 8., 4., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 4., 8., 1., 5., 1., 0.],\n",
       "         [0., 7., 5., 2., 2., 1., 0.],\n",
       "         [0., 0., 2., 1., 4., 3., 0.],\n",
       "         [0., 2., 4., 7., 5., 3., 0.],\n",
       "         [0., 1., 4., 8., 3., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 3., 5., 0., 3., 8., 0.],\n",
       "         [0., 5., 4., 7., 8., 4., 0.],\n",
       "         [0., 3., 8., 1., 2., 8., 0.],\n",
       "         [0., 7., 7., 2., 7., 5., 0.],\n",
       "         [0., 5., 2., 1., 0., 4., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Input_padded # вид тензора после паддинга"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 2, 2])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_func = conv_simple(Input, W, b, stride, padding)\n",
    "# res_func = conv_depthwise_separable(Input, W_depthwise, W_pointwise, b_pointwise, stride, padding)\n",
    "\n",
    "res_func.shape # размерность результата свёртки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[473., 665.],\n",
       "         [821., 901.]],\n",
       "\n",
       "        [[549., 631.],\n",
       "         [670., 827.]],\n",
       "\n",
       "        [[472., 563.],\n",
       "         [695., 839.]],\n",
       "\n",
       "        [[564., 667.],\n",
       "         [740., 948.]],\n",
       "\n",
       "        [[432., 577.],\n",
       "         [603., 848.]],\n",
       "\n",
       "        [[370., 561.],\n",
       "         [649., 743.]]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_func # результат свёртки"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
